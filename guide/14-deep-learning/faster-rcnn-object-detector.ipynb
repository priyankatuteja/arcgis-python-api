{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Faster R-CNN Object Detector"]}, {"cell_type": "markdown", "metadata": {"toc": true}, "source": ["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n", "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Faster-R-CNN-Object-Detector\" data-toc-modified-id=\"Faster-R-CNN-Object-Detector-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Faster R-CNN Object Detector</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Earlier-works\" data-toc-modified-id=\"Earlier-works-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Earlier works</a></span><ul class=\"toc-item\"><li><span><a href=\"#R-CNN\" data-toc-modified-id=\"R-CNN-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>R-CNN</a></span></li><li><span><a href=\"#Fast-R-CNN\" data-toc-modified-id=\"Fast-R-CNN-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Fast R-CNN</a></span></li></ul></li><li><span><a href=\"#Faster-R-CNN\" data-toc-modified-id=\"Faster-R-CNN-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Faster R-CNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Region-Proposal-Network-(RPN)\" data-toc-modified-id=\"Region-Proposal-Network-(RPN)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Region Proposal Network (RPN)</a></span></li></ul></li><li><span><a href=\"#Implementation-in-arcgis.learn\" data-toc-modified-id=\"Implementation-in-arcgis.learn-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Implementation in <code>arcgis.learn</code></a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>References</a></span></li></ul></li></ul></div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have seen how the one-shot object detection models such as [SSD](https://developers.arcgis.com/python/guide/how-ssd-works/), [RetinaNet](https://developers.arcgis.com/python/guide/how-retinanet-works/), and YOLOv3 work. However, before the single-stage detectors were the norm, the most popular object detectors were from the multi-stage R-CNN family. First, there was R-CNN, then Fast R-CNN came along with some improvements, and then eventually, Faster R-CNN became the state-of-the-art multi-stage object detector. As is obvious from the names, these models evolved from one to the next, providing better performance at faster speeds. Although Faster R-CNN is not as fast as some of the later single-stage models, it remains one of the most accurate object detection models."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/fasterrcnn-detections.jpg\" /></center>\n", "<center>Figure 1. Object Detection using Faster R-CNN [1]</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Earlier works"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### R-CNN"]}, {"cell_type": "markdown", "metadata": {}, "source": ["R-CNN (Regions with Convolutional Neural Networks) architecture is a combination of multiple algorithms put together. It first uses a selection search algorithm to select 2000 region proposals that might contain objects. Each of these region proposals or Regions of Interest (RoIs) is processed through a convolutional net to obtain feature maps. The feature maps are then passed to an SVM model to classify the object class and to a regression model to obtain tight bounding boxes [2]. This method, although novel at the time, is extremely slow. You can read more about R-CNN [here](https://towardsdatascience.com/r-cnn-for-object-detection-a-technical-summary-9e7bfa8a557c)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/rcnn.jpg\" /></center>\n", "<center>Figure 2. R-CNN Architecture [2]</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Fast R-CNN"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fast R-CNN came in as an improvement over R-CNN. In this model, instead of feeding each of the 2000 regions to separate CNNs, the whole image is fed to a single CNN. This results in a combined feature map for all the regions of interest. Region proposals are selected using an algorithm similar to the one used in R-CNN. An RoI pooling layer is used to extract and resize the feature maps of all the region proposals to the same size. This is then passed on to fully connected layers having two branches - a softmax classifier to give probabilities for each class and a bounding box regressor for precise bounding box coordinates [3]. This design speeds up the object detection task in comparison to R-CNN but still isn't good enough to work with large datasets. You can read more about Fast R-CNN [here](https://towardsdatascience.com/fast-r-cnn-for-object-detection-a-technical-summary-a0ff94faa022)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/fastrcnn.jpg\" /></center>\n", "<center>Figure 3. Fast R-CNN Architecture [3]</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Faster R-CNN"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Until Faster R-CNN came out, its contemporaries were using various algorithms for region proposal that were being computed on the CPU and creating a bottleneck. Faster R-CNN improved the object detection architecture by replacing the selection search algorithm in Fast R-CNN with a convolutional network called the Region Proposal Network (RPN). The rest of the model architecture remains the same as Fast R-CNN - the image is fed to a CNN to produce a feature map from which features for regions proposed by the RPN are selected and resized by a pooling layer and fed to an FC layer with two heads, a softmax classifier, and a bounding box regressor. This design increased the speed of detection and brought it closer to realtime [1]."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/fasterrcnn.jpg\" /></center>\n", "<center>Figure 4. Faster R-CNN Architecture [1]</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Region Proposal Network (RPN)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Region Proposal Network like other region proposal algorithms inputs an image and returns regions of interest that contain objects. An RPN also returns an objectness score that measures how likely the region is to have an object vs. a background [1]. In Faster R-CNN, the RPN and the detect network share the same backbone. The last shared layer of the backbone provides a feature map of the image that is used by the RPN to propose regions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/rpn.jpg\" /></center>\n", "<center>Figure 5. Region Proposal Network [1]</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using a sliding window approach, a small network is overlaid onto the feature map. At each spatial window, there are multiple anchor boxes that are of predetermined scales and aspect ratios. This enables the model to detect objects of a wide range of scales and aspect ratios in the same image. Usually, three different scales and three different aspect ratios are used resulting in nine anchor boxes at each spatial location that denotes the maximum region proposals at that spatial location. The small network then feeds into fully connected layers with two heads - one for the objectness score and the other for the bounding box coordinates of the region proposals. Figure 5, demonstrates the sliding window at a spatial location, the corresponding anchor boxes and how it feeds into the subsequent layers [1]. Note that these layers though similar to the last few layers of the Fast R-CNN object detector are not the same nor do they share the weights. The RPN classifies the regions in a class agnostic manner as it is tasked with only finding the regions which contain the objects.\n", "\n", "In this guide, we have tried to present an intuition to understand the Faster R-CNN model. To dive into more details about the model and it's architecture, please read the original papers mentioned in references below. You can reference this [blog](https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implementation in `arcgis.learn`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can create a Faster R-CNN model in `arcgis.learn` using a single line of code.  \n", "```\n", "model = FasterRCNN(data)\n", "```\n", "Where ``data`` is the databunch that you would have prepared using `prepare_data` function.\n", "Optionally, a ``backbone`` model from the ResNet family can be provided. The default is set to `resnet50`.\n", "\n", "For more information about the API, please go to the [API reference](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc/#fasterrcnn)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## References"]}, {"cell_type": "markdown", "metadata": {}, "source": ["[1] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun: \u201cFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\u201d, 2015; [http://arxiv.org/abs/1506.01497 arXiv:1506.01497].\n", "\n", "[2] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik: \u201cRich feature hierarchies for accurate object detection and semantic segmentation\u201d, 2013; [http://arxiv.org/abs/1311.2524 arXiv:1311.2524].\n", "\n", "[3] Ross Girshick: \u201cFast R-CNN\u201d, 2015; [http://arxiv.org/abs/1504.08083 arXiv:1504.08083]."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.2"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": true, "toc_position": {}, "toc_section_display": true, "toc_window_display": true}}, "nbformat": 4, "nbformat_minor": 4}